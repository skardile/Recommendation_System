<%@ page language="java" contentType="text/html; charset=ISO-8859-1"
    pageEncoding="ISO-8859-1"%>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>Insert title here</title>
</head>
<body>
<h2>Crawling</h2>
<p>Jsoup library has been used to crawl Java programming wikibooks.</p>
<p>I have crawled the content section wise, that means if we are crawling a webpage on Arrays then the crawler will collect information 
on Sections like Array, fundamentals,two-dimensional array and multidimensional array </p>
<p>For all of these different sections, different text files will be generated that include sub topic name, its explanation and code fragment,
all separated</p>

<h2>Indexing</h2>
<p>Lucene 5.4.1 has been used to index the crawled content</p>
<p>While indexing Lucene breaks down the document into number of terms. These terms are generated by StandardAnalyzer and I have used 
English Stop words filter while indexing</p>
<p>These terms are then stored into an index file where each terms are associated with respective documents which in my case are different sections
represented in text files</p>
<h2>Searching</h2>
<p>After passing entire post as a query, Lucene searcher returns the top 10 list of documents which has highest tf*idf score</p>

<h2>Stemming</h2>
<p>PorterFilter has been used to stem the input post</p>

<h2>Stemmed Key word finder</h2>
<p>Oracle Java Glossary is first read,<strong>stemmed</strong> and then stored into a hashmap </p>
<p>After that <strong>stemmed</strong> input posts are matched with this hashmap to find the Java keywords present in the post</p>
</body>
</html>